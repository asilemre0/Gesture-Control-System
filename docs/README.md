<h1 align="center">Science Week Project ðŸ›¸ðŸ”­</h1>


<br/><h3>
  Gesture Sense Interface - Control System
</h3>

<p align="left"> As a basic computer setup have essential components such as monitor, CPU , keyboard and mouse. <img width="400px" height="auto" align="right" src="https://user-images.githubusercontent.com/37477845/102235423-aa6cb680-3f35-11eb-8ebd-5d823e211447.jpg" style="margin:25px"/>
<br/>In morden days many PCs and Laptops comes with Touch Screen displays which is quite good.But For now let's go a step further , What about if we can control our display without 
touching it? Seems awesome right?
<br/><br/>We have just tried to make a system that can fingure out coordinate of our finger or say it just track position of our hand and then tracks the cursors location in the screen and matches it.
<br/>Through this we can create a virtual mouse with we can control system gesture without any 
physical contact. Let's know how it works and it's components.
</p>





<br/><br/>


<h3>Project Objectives : </h3>
<p>In this project we have tried our best to create an interactive control system through which we can replace the use of mouse and touch screen. Still currently this have many limitations but itâ€™ll be improved in further days.</p>

<br/><br/>

<h3 align="center">Libraries Used </h3>

  ```sh
     pip install opencv-python
     pip install mediapipe
     pip install pyautogui
 ```





<br/><br/>
<h3>How it works?</h3>
<p>
   The code uses OpenCV, MediaPipe, and PyAutoGUI libraries to create a virtual mouse. The program detects hand landmarks using MediaPipe Hands, and then maps the position of the index finger and thumb to the screen coordinates using PyAutoGUI.The index finger is used as a cursor, while the thumb is used as a clicker.
</p>





<br/><br/>
<h3>Code Explanation ~ </h3> 

<p>Step 1 : The program first imports the required libraries: cv2 (OpenCV), mediapipe, and pyautogui.</p>

```py
 importÂ cv2 
 importÂ mediapipeÂ asÂ mp 
 importÂ pyautogui

 ```

<p>Step 2 : It then sets up the camera capture and initializes the MediaPipe Hands object and drawing utilities.</p>

```py
 capÂ =Â cv2.VideoCapture(0) 
 hand_detectorÂ =Â mp.solutions.hands.Hands() 
 drawing_utilsÂ =Â mp.solutions.drawing_utils 

 ```


<p>Step 3 : The program then gets the screen dimensions using PyAutoGUI.</p>

```py
 screen_width,Â screen_heightÂ =Â pyautogui.size() 
 index_yÂ =Â 0


 ```


<p>Step 4 : In the main loop, the program reads the video stream from the camera and flips it horizontally.</p>

```py
 whileÂ True: 
 Â Â Â Â _,Â frameÂ =Â cap.read() 
 Â Â Â Â frameÂ =Â cv2.flip(frame,Â 1)

 ```


<p>Step 5 : The program then converts the BGR image to RGB and passes it to the MediaPipe Hands object for hand landmark detection.</p>


```py
 Â Â Â Â frame_height,Â frame_width,Â _Â =Â frame.shape 
 Â Â Â Â rgb_frameÂ =Â cv2.cvtColor(frame,Â cv2.COLOR_BGR2RGB) 
 Â Â Â Â outputÂ =Â hand_detector.process(rgb_frame) 
 Â Â Â Â handsÂ =Â output.multi_hand_landmarks

 ```


<p>Step 6 : If hands are detected, the program loops through each hand and draws the landmarks on the frame.
</p>

```sh 
 Â Â Â Â ifÂ hands: 
 Â Â Â Â Â Â Â Â forÂ handÂ inÂ hands: 
 Â Â Â Â Â Â Â Â Â Â Â Â drawing_utils.draw_landmarks(frame,Â hand) 
 Â Â Â Â Â Â Â Â Â Â Â Â landmarksÂ =Â hand.landmark 
 ```


<p>Step 7 : The program then extracts the landmark positions and maps the position of the index finger and thumb to the screen coordinates.</p>

```sh 
  Â Â Â Â Â Â Â Â Â Â Â Â forÂ id,Â landmarkÂ inÂ enumerate(landmarks): 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â xÂ =Â int(landmark.x*frame_width) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â yÂ =Â int(landmark.y*frame_height) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ idÂ ==Â 8: 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.circle(img=frame,Â center=(x,y),Â radius=10,Â color=(0,Â 255,Â 255)) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â index_xÂ =Â screen_width/frame_width*x 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â index_yÂ =Â screen_height/frame_height*y
 ```


<br/> Finally, the program checks the distance between the index finger and thumb, and performs a click or moves the cursor accordingly.


```py
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ idÂ ==Â 8: 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.circle(img=frame,Â center=(x,y),Â radius=10,Â color=(0,Â 255,Â 255)) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â index_xÂ =Â screen_width/frame_width*x 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â index_yÂ =Â screen_height/frame_height*y 
  
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ idÂ ==Â 4: 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.circle(img=frame,Â center=(x,y),Â radius=10,Â color=(0,Â 255,Â 255)) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â thumb_xÂ =Â screen_width/frame_width*x 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â thumb_yÂ =Â screen_height/frame_height*y 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â print('outside',Â abs(index_yÂ -Â thumb_y)) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ abs(index_yÂ -Â thumb_y)Â <Â 20: 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pyautogui.click() 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pyautogui.sleep(1) 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â elifÂ abs(index_yÂ -Â thumb_y)Â <Â 100: 
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pyautogui.moveTo(index_x,Â index_y) 
 Â Â Â Â cv2.imshow('VirtualÂ Mouse',Â frame) 
 Â Â Â Â cv2.waitKey(1)
 ```



